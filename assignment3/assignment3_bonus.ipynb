{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "assignment3_bonus.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPM3XOpnx6qf",
        "colab_type": "text"
      },
      "source": [
        "# Assignment 3 option 2: ConvNet\n",
        "In this assignment you will train a ConvNet to predict the language of a\n",
        "surname from its spelling."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ToNoisdGjRuM",
        "colab_type": "code",
        "outputId": "804e576b-5e27-440b-bed0-8ea73e64a3f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "!pip install ipython-autotime\n",
        "%load_ext autotime"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting ipython-autotime\n",
            "  Downloading https://files.pythonhosted.org/packages/e6/f9/0626bbdb322e3a078d968e87e3b01341e7890544de891d0cb613641220e6/ipython-autotime-0.1.tar.bz2\n",
            "Building wheels for collected packages: ipython-autotime\n",
            "  Building wheel for ipython-autotime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ipython-autotime: filename=ipython_autotime-0.1-cp36-none-any.whl size=1832 sha256=f24b73325d2f124b52003c4ce0e2333d016fbe020bd4dd7cb2d33ee3c2c6abbc\n",
            "  Stored in directory: /root/.cache/pip/wheels/d2/df/81/2db1e54bc91002cec40334629bc39cfa86dff540b304ebcd6e\n",
            "Successfully built ipython-autotime\n",
            "Installing collected packages: ipython-autotime\n",
            "Successfully installed ipython-autotime-0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33B0OGvt1W8_",
        "colab_type": "code",
        "outputId": "e02e0710-9c23-4b64-b0ad-1ccd91e8d723",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import pandas as pd\n",
        "import random\n",
        "from google.colab import drive\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import sparse\n",
        "from collections import Counter\n",
        "from operator import itemgetter\n",
        "from random import sample "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 349 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z15WVr0ZypAu",
        "colab_type": "text"
      },
      "source": [
        "# Component 1: Preparing Data\n",
        "Read in the training data, determine the maximum\n",
        "length of all the names in the dataset, determine the number of unique\n",
        "characters in the text and set up mapping functions - one mapping\n",
        "each character to a unique index and another mapping each index to a character.\n",
        "\n",
        "## 0.1 Read in the data and get it ready\n",
        "read in acsii_names.txt\n",
        "put all names and ther labels un a call array all_names and a vector y \n",
        "convert all uppercase letters to lowercase"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8si6my0zxmUI",
        "colab_type": "code",
        "outputId": "f3d36ae4-110f-4633-bf92-701ac0b488f1",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "# upload txt file to colab: https://buomsoo-kim.github.io/colab/2018/04/15/Colab-Importing-CSV-and-JSON-files-in-Google-Colab.md/\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-22ae3549-baf6-4a7f-8140-de8c30c80571\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-22ae3549-baf6-4a7f-8140-de8c30c80571\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving ascii_names.txt to ascii_names.txt\n",
            "Saving Validation_Inds.txt to Validation_Inds.txt\n",
            "time: 17 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqRt3dTiHIVD",
        "colab_type": "code",
        "outputId": "8db2d11c-edd6-4cf1-f7f1-e4f58af70644",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# preporocess data\n",
        "names_train = []\n",
        "names_val = []\n",
        "labels_train = []\n",
        "labels_val = []\n",
        "\n",
        "name_data = \"ascii_names.txt\"\n",
        "validation_data = \"Validation_Inds.txt\"\n",
        "names = uploaded[name_data].decode(\"utf-8\").split(\"\\n\")\n",
        "validation_idx = uploaded[validation_data].decode(\"utf-8\").split()\n",
        "names = list(filter(lambda x: x != \"\", names))\n",
        "for i in range(len(names)):\n",
        "  line = names[i].split(\",\")[0].rsplit(' ', 1)\n",
        "  if len(line)>1 and line[1] != '':\n",
        "    if str(i+1) in validation_idx: \n",
        "      names_val.append(line[0].lower())\n",
        "      labels_val.append(int(line[1]))\n",
        "    else:\n",
        "      names_train.append(line[0].lower())\n",
        "      labels_train.append(int(line[1]))\n",
        "\n",
        "print(len(names_val), len(names_train), len(labels_val), len(labels_train))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "252 19798 252 19798\n",
            "time: 137 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuF8uLtILOaR",
        "colab_type": "code",
        "outputId": "1675143a-c4b5-4d6d-bf07-65f0c9a93fd8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "C = sorted(set([i for ele in names_train for i in ele])) # set of unique characters in names\n",
        "d = len(C) # number of unique characters in names\n",
        "n_len =  len(max(names_train, key=len)) # size of longest name in names\n",
        "K = len(set(labels_train)) # number of classes\n",
        "char_to_ind =  { val : id for id,val in enumerate(C) } # dict with char and index in set C\n",
        "print(n_len)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "19\n",
            "time: 44.5 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sK_HSq6Fg0So",
        "colab_type": "code",
        "outputId": "6bcdf40f-3505-4c70-bb82-930946e1bcf0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "def createOneHot_name(_name, _char_to_ind, _n_len, _d):\n",
        "  values =  [_char_to_ind[val] for val in list(_name)]\n",
        "  one_hot = np.eye(_d)[values].copy()\n",
        "  one_hot.resize((_n_len, _d),  refcheck=False)\n",
        "  return one_hot.T \n",
        "createOneHot_name('abc', char_to_ind, 4, 6).flatten('F')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1.,\n",
              "       0., 0., 0., 0., 0., 0., 0.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        },
        {
          "output_type": "stream",
          "text": [
            "time: 7.72 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4yzFcpj0oaE",
        "colab_type": "code",
        "outputId": "f4110678-eebe-44cf-eedf-6cf6d0063bc5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# create X as one hot encoding for names: columns represent names, rows the vectorized information\n",
        "def createOneHot_X(names):\n",
        "  X = np.zeros((d*n_len, len(names)))\n",
        "  for id,name in enumerate(names):\n",
        "    X[:,id] = createOneHot_name(name, char_to_ind, n_len, d).flatten('F')\n",
        "  return X\n",
        "\n",
        "print(createOneHot_X(names_train).shape, len(names_train))\n",
        "print(createOneHot_X(names_val).shape, len(names_val))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(532, 19798) 19798\n",
            "(532, 252) 252\n",
            "time: 395 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8rWIOTvzYPt",
        "colab_type": "code",
        "outputId": "b515c1be-2f5b-41ff-d2eb-137edfea1e59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# create one hot encoding for labels --> each column corresponds to one name\n",
        "def createOneHot_Y(labels):\n",
        "  onehot_encoder = OneHotEncoder(sparse=False, categories='auto')\n",
        "  Y = onehot_encoder.fit_transform(np.array(labels).reshape(-1,1)).astype(int).T\n",
        "  return Y\n",
        "print(createOneHot_Y(labels_train).shape)\n",
        "print(createOneHot_Y(labels_val).shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(18, 19798)\n",
            "(18, 252)\n",
            "time: 20.5 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ep2ITSlj-rzY",
        "colab_type": "code",
        "outputId": "60df1844-76ee-46f2-9275-ee8fc6e0a460",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# plot all graphs\n",
        "def plotResults(costs_train, accs_train, costs_validate, accs_validate, safeto, save=False):\n",
        "    # plotting costs\n",
        "    plt.plot(costs_train, label = \"training\")\n",
        "    plt.plot(costs_validate, label = \"validation\")\n",
        "    plt.xlabel('saving steps')\n",
        "    plt.ylabel('cost')\n",
        "    plt.suptitle('Cost')\n",
        "    plt.legend()\n",
        "    if save:\n",
        "        plt.savefig('Result_Pics/'+safeto+'cost.png')\n",
        "    plt.show()  \n",
        "    \n",
        "    # plotting accuracy\n",
        "    plt.plot(accs_train, label = 'training')\n",
        "    plt.plot(accs_validate, label = 'validation')\n",
        "    plt.xlabel('saving steps')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.suptitle('Accuracy')\n",
        "    plt.legend()\n",
        "    if save:\n",
        "        plt.savefig('Result_Pics/'+safeto+'acc.png')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 7.57 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHUI13prORyp",
        "colab_type": "text"
      },
      "source": [
        "# 2 Convolutional Network\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eb2_kuwnguYT",
        "colab_type": "code",
        "outputId": "0625c037-079d-498b-f8ec-411140866282",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# object for parameters \n",
        "random.seed( 30 )\n",
        "\n",
        "class ConvNet():\n",
        "  def __init__(self, n_classes, eta, rho, ns, ks, d, n_len, K, L):\n",
        "    self.L = L # number of conv layers\n",
        "    self.n = ns # filters applied at layer  []\n",
        "    self.k = ks # the width of the filters applied at layer []\n",
        "    self.n_len = n_len\n",
        "    self.d = d\n",
        "    self.F = {}\n",
        "    self.eta = eta\n",
        "    self.rho = rho\n",
        "    self.M_filter = {}\n",
        "    self.len_f = [] # lens used for calculation MF matrix for each layer []\n",
        "    self.K = K\n",
        "    self.confusion = []\n",
        "\n",
        "    self.len_f.append(n_len)\n",
        "    for i in range(L): # for each layer\n",
        "      # n len f\n",
        "      n_len_new =  self.len_f[-1]-ks[i]+1\n",
        "      self.len_f.append(n_len_new)\n",
        "      # filters\n",
        "      if i == 0:\n",
        "        self.F[i] = np.zeros((self.d, self.k[i], self.n[i]) ) \n",
        "      else:\n",
        "        self.F[i] = np.zeros((self.n[i-1], self.k[i], self.n[i]) )\n",
        "    self.W = np.zeros((n_classes, self.len_f[-1] * self.n[-1]))  # K\u0002n2\u0003nlen2\n",
        "\n",
        "  def init(self,mu, sigma):\n",
        "    # init filters\n",
        "    for i in range(self.L): # for every layer\n",
        "        self.F[i] = np.random.normal(mu, sigma, self.F[i].shape ) \n",
        "    # init W\n",
        "    self.W = np.random.normal(mu, sigma, self.W.shape) \n",
        "\n",
        "  def makeMFMatrix(self, F, n_len): # called for a specific layer - nothing to change\n",
        "    (d, k, nf) = F.shape\n",
        "    M_filter = np.zeros(((n_len -k+1)*nf, n_len*d))\n",
        "    Vec_filter = F.reshape((d*k, nf), order = 'F').T\n",
        "    #print (F.shape, Vec_filter.shape, M_filter.shape)\n",
        "\n",
        "    for i in range(n_len - k + 1):\n",
        "      row_start = i * nf\n",
        "      row_end = (i + 1) * nf\n",
        "      col_start = d * i\n",
        "      col_end = d * i + d * k      \n",
        "      M_filter [row_start:row_end,col_start:col_end] = Vec_filter\n",
        "    \n",
        "    return M_filter\n",
        "\n",
        "  def makeMXMatrix(self, x_input, d, k, nf, stride = 1):    #d,k,nf = size(F) # not used in improves version - nothing to change\n",
        "    n_len = int(len(x_input)/d)\n",
        "    M_input = np.zeros((nf*(n_len -k+1),nf*d*k))\n",
        "    x_input = x_input.reshape((d, n_len), order='F') # todo: try to get a version without all the shaping\n",
        "    \n",
        "    for i in range((n_len -k+1)):\n",
        "      row_start = i * nf   \n",
        "      vec =  (x_input[:, i:k+i].reshape((d*k, 1), order='F')).T \n",
        "      #vec = x_input[i*d*stride: k*d*stride+i*d*stride]\n",
        "      for j in range(nf):     \n",
        "        M_input [row_start+j, j*d*k: (j+1)*d*k] = vec\n",
        "  \n",
        "    return M_input\n",
        "\n",
        "  ''' np.kron is very slow in this loop. Do not use it! Use makeMXMatrix instead ''' # not used in improves version - nothing to change\n",
        "  def makeMXMatrixKron(self, x_input, d, k, nf, stride = 1):    #F.shape = d x k x nf  , d = 28, k&nf to decide\n",
        "    n_len = int(len(x_input)/d)               \t    # X.shape = d x n_len\n",
        "    M_input = np.zeros((nf*(n_len -k+1),nf*d*k))    # size of MX\n",
        "    vec = np.zeros(((n_len -k+1),d*k))              # one vector square\n",
        "\n",
        "    x_input = x_input.reshape((d, n_len), order='F') # todo: try to get a version without all the shaping\n",
        "    print(x_input.shape, d, k, nf, n_len, (n_len -k+1), M_input.shape )\n",
        "\n",
        "    for i in range((n_len -k+1)):\n",
        "      vec[i] =  (x_input[:, i:k+i].reshape((d*k, 1), order='F')).T \n",
        "      #vec[i]= x_input[i*d*stride: k*d*stride+i*d*stride]\n",
        "      if i == 0:\n",
        "        M_input =  np.kron(np.eye(nf),vec[i])\n",
        "      else:\n",
        "        M_input = np.vstack((M_input, np.kron(np.eye(nf),vec[i])))\n",
        "  \n",
        "    return M_input\n",
        "\n",
        "  def makeMXMatrixVec(self, x_input, d, k):   \n",
        "    n_len = int(len(x_input)/d)\n",
        "    vec = np.zeros(((n_len -k+1),d*k))\n",
        "    x_input = x_input.reshape((d, n_len), order='F') \n",
        "    \n",
        "    for i in range((n_len -k+1)):\n",
        "      vec[i] =  (x_input[:, i:k+i].reshape((d*k, 1), order='F')).T \n",
        "  \n",
        "    return vec\n",
        "\n",
        "  def softmax(self, scores): # not layer specific - nothing to change\n",
        "    f = np.exp(scores - np.max(scores))  # avoiding nan for large numbers\n",
        "    return f / f.sum(axis=0)\n",
        "\n",
        "  def evaluateClassifier(self, X_batch, MFs, W): # forward pass\n",
        "    X_batches = [X_batch]\n",
        "    for i in range(len(MFs)): # iterate over MFs - one MF entry for each layer \n",
        "      new_X_batch = np.maximum(np.dot(MFs[i],X_batches[i]), 0)\n",
        "      X_batches.append(new_X_batch)\n",
        "\n",
        "    # fully connected\n",
        "    S_batch = np.dot(W ,X_batches[-1])\n",
        "    # Apply the SoftMax operator to each column\n",
        "    P_batch = self.softmax(S_batch)\n",
        "\n",
        "    return X_batches, P_batch # return list of x_batches instead of 1 and 2 - keep in mind [0] is initial xBatch, [1] x_batch1, [i] x_batchi\n",
        "\n",
        "  def crossEntropyLoss(self, X, Y, MFs, W):\n",
        "    _, P = self.evaluateClassifier(X, MFs, W)\n",
        "    Y = np.reshape(Y, (P.shape))\n",
        "    loss = -(np.multiply(Y, np.log(P)))  # check this:  - np.sum(Y*np.log(P)), removed  np.sum\n",
        "    return loss\n",
        "\n",
        "  def cost(self, X, Y, MFs, W):\n",
        "    J = np.sum(self.crossEntropyLoss(X, Y, MFs, W)) / X.shape[1]\n",
        "    return J\n",
        "\n",
        "  def computeAccuracy(self, X, labels, MFs, W):\n",
        "    _, P = self.evaluateClassifier(X, MFs, W)\n",
        "    pred = np.argmax(P, axis=0)\n",
        "    pred = [x + 1 for x in pred]\n",
        "    #print(pred)\n",
        "    acc = np.count_nonzero(np.array(pred) == np.array(labels)) / X[1].size\n",
        "    return acc\n",
        "\n",
        "  # old: def passBackward(self, X_batch, X_batch1, X_batch2, P_batch, Y_batch, MFs, MX_prec):\n",
        "  def passBackward(self, X_batches, P_batch, Y_batch, MFs, MX_prec):\n",
        "    # assume forward pass / evaluate classifier is done before\n",
        "    grad_W = np.zeros(self.W.shape)\n",
        "    grad_F = []\n",
        "    for i in range(len(self.F)): # layer specific\n",
        "      grad_F.append(np.zeros(self.F[i].shape))\n",
        "    \n",
        "    # fully conected layer\n",
        "    G_batch = - (Y_batch - P_batch)\n",
        "    grad_W = np.dot(G_batch, X_batches[-1].T) / X_batches[-1].shape[1] # use last one in list\n",
        "\n",
        "    '''generalize from here'''\n",
        "\n",
        "    # for each layer going backwards\n",
        "    for i in range(len(self.F)-1, -1, -1):\n",
        "      # Propagate the gradient to the previous layer through the fully conected /convolutional layer and ReLu operation:\n",
        "      if i == (len(self.F)-1): # fully conected - can be changes to make it more general\n",
        "        G_batch = np.dot(self.W.T, G_batch)\n",
        "      else:\n",
        "        G_batch = np.dot(MFs[i+1].T, G_batch) # MF from previous layer\n",
        "      \n",
        "      G_batch = np.multiply(G_batch, np.where(X_batches[i+1] >0, 1,0)) # from previous\n",
        "\n",
        "      # compute gradients w.r.t the layer\n",
        "      for j in range(X_batches[i].shape[1]): # todo\n",
        "        gj = G_batch[:,j]\n",
        "        xj = X_batches[i][:,j] # todo\n",
        "\n",
        "        # improvement 2\n",
        "        if i == 0: \n",
        "          M_inputj_vec = self.makeMXMatrixVec(xj, self.d, self.k[i]) # input dimension for first layer is d\n",
        "        else: \n",
        "          M_inputj_vec = self.makeMXMatrixVec(xj, self.n[i-1], self.k[i] ) # input dimension for other layers if previous n\n",
        "\n",
        "        leni = int( gj.shape[0] / self.n[i]) # first dim of g_new         # todo??? review this n\n",
        "        g_efficient = gj.reshape(leni, self.n[i])    # todo??? review this n\n",
        "        v2 = np.dot(M_inputj_vec.T, g_efficient) \n",
        "        grad_F[i] += v2.reshape(grad_F[i].shape, order='F') / X_batches[i].shape[1]     # todo grad_F and x_batch\n",
        "\n",
        "    return grad_W, grad_F\n",
        "    \n",
        "  def num_grads(self, X, Y, MF, W, h):\n",
        "    '''Translation from Matlab template provided by classmate (Many thanks to Filip)'''\n",
        "    dW = np.zeros_like(W)\n",
        "    (a, b) = W.shape\n",
        "    for i in range(a):\n",
        "        for j in range(b):\n",
        "            C = []\n",
        "            \n",
        "            for m in [-1, 1]:\n",
        "                W_try = np.copy(W)\n",
        "                W_try[i, j] += m * h\n",
        "                C.append(self.cost(X, Y, MF, W_try))\n",
        "            dW[i, j] = (C[1] - C[0]) / (2 * h)\n",
        "\n",
        "    dF = [np.zeros(self.F[f].shape) for f in self.F]\n",
        "    for i in range(len(self.F)):\n",
        "        (a, b, c) = self.F[i].shape\n",
        "        for j in range(a):\n",
        "            for k in range(b):\n",
        "                for q in range(c):\n",
        "                    C = []\n",
        "\n",
        "                    for m in [-1, 1]:\n",
        "                        Fi_try = np.copy(self.F[i])\n",
        "                        Fi_try[j, k, q] += m * h\n",
        "                        MFi_try = self.makeMFMatrix(Fi_try, self.len_f[i])\n",
        "                        MF_lst = []\n",
        "                        for ii in range(len(self.F)):\n",
        "                          MF_lst.append(MFi_try if ii == i else MF[ii])\n",
        "                        C.append(self.cost(X, Y, MF_lst, W))\n",
        "                    dF[i][j, k, q] = (C[1] - C[0]) / (2 * h)\n",
        "    return dW, dF\n",
        "    \n",
        "  def computeRelativeError(self, ga ,gn ,eps):\n",
        "    return np.absolute(np.subtract(ga, gn))/np.maximum(np.add(np.absolute(ga), np.absolute(gn)), np.full(ga.shape, eps))\n",
        "  \n",
        "  def generateMiniBatches(self, n_batch, X, Y):\n",
        "    n = X[1].size\n",
        "    X_batches = []\n",
        "    Y_batches = []\n",
        "    batch_index = []\n",
        "\n",
        "    for j in range(int(n / n_batch)):\n",
        "      j_start = j * n_batch\n",
        "      j_end = (j + 1) * n_batch\n",
        "\n",
        "      X_batch = X[:, j_start:j_end]\n",
        "      Y_batch = Y[:, j_start:j_end]\n",
        "\n",
        "      X_batches.append(X_batch)\n",
        "      Y_batches.append(Y_batch)\n",
        "      batch_index.append(( j_start,j_end))\n",
        "\n",
        "    return X_batches, Y_batches, batch_index\n",
        "\n",
        "  def miniBatchGD(self, X, Y,  X_val, Y_val, labels_train, labels_val, n_batch, n_update, n_epochs, balanced):\n",
        "    costs_train = []\n",
        "    accs_train = []\n",
        "    costs_val = []\n",
        "    accs_val = []\n",
        "    MFs =[]\n",
        "    t = 0\n",
        "    #F = self.F.copy()\n",
        "    v_W = np.zeros(self.W.shape)\n",
        "    v_F = []\n",
        "   \n",
        "    # get indexes\n",
        "    index_dict = self.create_index_dict(labels_train)\n",
        "    \n",
        "    # construct M_filters\n",
        "    for i in range(len(self.F)):\n",
        "      MFs.append(self.makeMFMatrix(self.F[i], self.len_f[i]))\n",
        "      v_F.append(np.zeros(self.F[i].shape))\n",
        "\n",
        "     # precompute MX for the first layer for each batch  \n",
        "     # improvement 1 - not efficient\n",
        "    MX_prec = []\n",
        "    '''\n",
        "    for j in range(X.shape[1]):\n",
        "      xj = X[:,j]\n",
        "      MX_prec.append(sparse.csr_matrix(self.makeMXMatrix(xj, self.d, self.k[0], self.n[0])) ) \n",
        "\n",
        "    print(\"precomputing mx\")\n",
        "    MX_prec = [sparse.csr_matrix(self.makeMXMatrix(X[:,x], self.d, self.k[0], self.n[0]))  for x in range(X.shape[1])]\n",
        "    print(\"precomputing mx done \")'''\n",
        "\n",
        "    for epoch in range(n_epochs):  # iterate over epoch\n",
        "      #print(\"epoch\", epoch)\n",
        "\n",
        "      if balanced:\n",
        "        # get balanced data\n",
        "        X_balanced, y_balanced = self.createBalancedDataset(X, labels_train, index_dict, self.K)\n",
        "        #Y_balanced = onehot_encoder.fit_transform(np.array(y_balanced).reshape(-1,1)).astype(int).T\n",
        "        Y_balanced = createOneHot_Y(y_balanced)\n",
        "\n",
        "        # construct batches from balanced data\n",
        "        X_batches, Y_batches, batch_index = self.generateMiniBatches(n_batch, X_balanced, Y_balanced)\n",
        "      else: \n",
        "        # construct batches from unbalanced data\n",
        "        X_batches, Y_batches, batch_index = self.generateMiniBatches(n_batch, X, Y)\n",
        "\n",
        "      # construct MF prec for batches\n",
        "      # improvement 1 - not efficient\n",
        "      '''\n",
        "      MX_prec_batches = []\n",
        "      for i in range(len(X_batches)):\n",
        "        MX_prec_batches.append(MX_prec[batch_index[i][0]:batch_index[i][1]] ) '''\n",
        "\n",
        "      #MX_prec_batches = [MX_prec[batch_index[i][0]:batch_index[i][1]] for i in range(len(X_batches))]\n",
        "      \n",
        "      for idx, X_batch in enumerate(X_batches, start=0):  # iterate over mini-batches\n",
        "        Y_batch = Y_batches[idx]\n",
        "\n",
        "         #save acc and cost every n_update step\n",
        "        if t % (n_update-1) == 0:\n",
        "          costs_train.append(self.cost(X, Y, MFs, self.W))\n",
        "          accs_train.append(self.computeAccuracy(X, labels_train, MFs, self.W))\n",
        "          costs_val.append(self.cost(X_val, Y_val, MFs, self.W))\n",
        "          accs_val.append(self.computeAccuracy(X_val, labels_val, MFs, self.W))\n",
        "        \n",
        "        # pass forward\n",
        "        X_batches_fw, P_batch = self.evaluateClassifier(X_batch, MFs, self.W)\n",
        "\n",
        "        # compute gradients / pass backwards\n",
        "\n",
        "        #MX_prec_batch = MX_prec[batch_index[idx][0]:batch_index[idx][1]]\n",
        "        grad_W, grads_F = self.passBackward(X_batches_fw, P_batch, Y_batch, MFs,[])\n",
        "        #grad_W, grads_F = self.passBackward(X_batch, X_batch1, X_batch2, P_batch, Y_batch, MFs, MX_prec_batch) # improvement 1 - not efficient\n",
        "        \n",
        "        # momentum \n",
        "        # adjust learnung parameters \n",
        "        v_W = v_W * self.rho - self.eta * grad_W\n",
        "        self.W += v_W\n",
        "\n",
        "        # compute MFs\n",
        "        MFs = []\n",
        "        for i in range(len(self.F)): # layer specific       \n",
        "          v_F[i] = v_F[i] * self.rho - self.eta * grads_F[i]\n",
        "          self.F[i] += v_F[i]\n",
        "          MFs.append(self.makeMFMatrix(self.F[i], self.len_f[i]))\n",
        "  \n",
        "        # increase t\n",
        "        t += 1\n",
        "    \n",
        "    costs_train.append(self.cost(X, Y, MFs, self.W))\n",
        "    accs_train.append(self.computeAccuracy(X, labels_train, MFs, self.W))\n",
        "    costs_val.append(self.cost(X_val, Y_val, MFs, self.W))\n",
        "    accs_val.append(self.computeAccuracy(X_val, labels_val, MFs, self.W))\n",
        "\n",
        "    #print confusion matrix for train and validate\n",
        "    self.printConfusion(X, MFs, self.W, labels_train )\n",
        "    self.printConfusion(X_val, MFs, self.W, labels_val )\n",
        " \n",
        "    return [costs_train, accs_train], [costs_val, accs_val], self.W, self.F\n",
        "\n",
        "  def printConfusion(self, X, MFs, W, y ):\n",
        "    _, P = self.evaluateClassifier(X, MFs, W)\n",
        "    pred = np.argmax(P, axis=0)\n",
        "    pred = [x + 1 for x in pred]\n",
        "    # how to construct confusion matrix: https://stackoverflow.com/questions/2148543/how-to-write-a-confusion-matrix-in-python\n",
        "    y_actu = pd.Series(y, name='Actual')\n",
        "    y_pred = pd.Series(pred, name='Predicted')\n",
        "    df_confusion = pd.crosstab(y_actu, y_pred,  margins=False)\n",
        "    self.confusion.append(df_confusion)\n",
        "    #display(df_confusion)\n",
        "\n",
        "  def create_index_dict(self,  y_unbalanced):\n",
        "    # filter the indexes\n",
        "    indexes_for_classes = {} \n",
        "    for i, value in enumerate(y_unbalanced):\n",
        "      if value not in indexes_for_classes:\n",
        "        indexes_for_classes[value] = [i]\n",
        "        continue\n",
        "      indexes_for_classes[value].append(i)\n",
        "    return indexes_for_classes\n",
        "\n",
        "  def predict(self, X):\n",
        "    MFs_learned = []\n",
        "    for i in range(len(self.F)):\n",
        "      MFs_learned.append(conv.makeMFMatrix(self.F[i], self.len_f[i]))\n",
        "\n",
        "    X_OH = createOneHot_X(X)\n",
        "    _, P = conv.evaluateClassifier(X_OH, MFs_learned, self.W)\n",
        "    pred = np.argmax(P, axis=0)\n",
        "    pred = [x + 1 for x in pred]\n",
        "    \n",
        "    return pred\n",
        "\n",
        "\n",
        "  def createBalancedDataset(self, X_unbalanced, y_unbalanced, index_dict, K):\n",
        "    # using counter due to efficiency: https://stackoverflow.com/questions/2600191/how-can-i-count-the-occurrences-of-a-list-item, already sorted\n",
        "    _, min_count =  min(Counter(y_unbalanced).items(), key=itemgetter(1)) # getting min element out of it https://stackoverflow.com/questions/53046410/min-value-from-a-counter-result-in-python\n",
        "\n",
        "    X_balanced = np.zeros((X_unbalanced.shape[0], min_count*K)) \n",
        "    y_balanced = []\n",
        "    counter = 0\n",
        "    for label, indexes in index_dict.items():      # from each class indexes\n",
        "      sampled = sample(indexes,min_count ) # take a random with replacement of min size\n",
        "      for idx in sampled:\n",
        "        X_balanced[:,counter] = X_unbalanced[:,idx]\n",
        "        y_balanced.append(y_unbalanced[idx])\n",
        "        counter +=1\n",
        "\n",
        "    return X_balanced, y_balanced\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 1.45 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZqCR6Mgfi0-",
        "colab_type": "text"
      },
      "source": [
        "## 4 Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2US0L6B9rxyl",
        "colab_type": "text"
      },
      "source": [
        "## Bonus 1: Elaborating multiple convolutional layers and numbers of filters\n",
        "\n",
        "Layers: 2 to 8\n",
        "\n",
        "Fiters:  20, 30, 40"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "876SZAwOTXTb",
        "colab_type": "code",
        "outputId": "3c2546d2-c135-4070-eb63-5371b2d00b48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        }
      },
      "source": [
        "#Network definition\n",
        "# network parameters for tuning\n",
        "eta = 0.01\n",
        "rho = 0.9\n",
        "mu = 0\n",
        "sigma = np.sqrt(2 / d) # he init\n",
        "k=3\n",
        "\n",
        "n_batch = 100\n",
        "n_update = 500\n",
        "n_epochs = 100\n",
        "balanced = True\n",
        "\n",
        "X = createOneHot_X(names_train)\n",
        "Y = createOneHot_Y(labels_train)\n",
        "X_val = createOneHot_X(names_val)\n",
        "Y_val = createOneHot_Y(labels_val)\n",
        "\n",
        "#stroe results\n",
        "train_store=[]\n",
        "val_store=[]\n",
        "# layer specific parameters\n",
        "\n",
        "# iterate over number of layers\n",
        "for L in range(2, 9, 2):\n",
        "  # iterate over fiter sizes\n",
        "  for nfilter in range(20, 41, 10):\n",
        "    ns = [nfilter]*L\n",
        "    ks = [k]*L\n",
        "    print(\"Results for:\", L, nfilter)\n",
        "    # building object and initializing\n",
        "    conv = ConvNet( K, eta, rho, ns, ks, d, n_len, K, L)   \n",
        "    conv.init( mu, sigma) #layer1\n",
        "    \n",
        "    train, validate, W_learned, F_learned = conv.miniBatchGD( X, Y,  X_val, Y_val, labels_train, labels_val, n_batch, n_update, n_epochs, balanced)\n",
        "    train_store.append(train)\n",
        "    val_store.append(validate)\n",
        "    \n",
        "    print(np.min(train[0]), np.max(train[1]), train[0][-1], train[1][-1])\n",
        "    print(np.min(validate[0]), np.max(validate[1]), validate[0][-1], validate[1][-1])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Results for: 2 20\n",
            "1.569178315680376 0.5071724416607738 1.569178315680376 0.5071724416607738\n",
            "2.146757005958165 0.38492063492063494 2.14760170257085 0.38492063492063494\n",
            "Results for: 2 30\n",
            "1.522574562197274 0.5335387412869987 1.522574562197274 0.5335387412869987\n",
            "2.1559591319426614 0.38492063492063494 2.1559591319426614 0.38492063492063494\n",
            "Results for: 2 40\n",
            "1.4015795636697044 0.5772805333872109 1.4015795636697044 0.5772805333872109\n",
            "2.134975953703798 0.42857142857142855 2.134975953703798 0.4246031746031746\n",
            "Results for: 4 20\n",
            "1.3941087265263736 0.5737448227093646 1.4258030148517238 0.5660167693706435\n",
            "2.2788884454857166 0.38492063492063494 2.3128844887982645 0.373015873015873\n",
            "Results for: 4 30\n",
            "1.4773893276187222 0.5510152540660673 1.4912012045703384 0.547934134761087\n",
            "2.109145120938567 0.3888888888888889 2.109145120938567 0.3888888888888889\n",
            "Results for: 4 40\n",
            "1.2701610583247138 0.6263258915041924 1.2701610583247138 0.6263258915041924\n",
            "2.3036750420085137 0.4087301587301587 2.4528543408109122 0.4087301587301587\n",
            "Results for: 6 20\n",
            "1.5476322305052543 0.5125770279826245 1.5476322305052543 0.5125770279826245\n",
            "2.354514387356316 0.39285714285714285 2.354514387356316 0.376984126984127\n",
            "Results for: 6 30\n",
            "1.2403981758623221 0.6332962925548035 1.2403981758623221 0.6332962925548035\n",
            "2.421733416000478 0.4087301587301587 2.5837775436222135 0.3888888888888889\n",
            "Results for: 6 40\n",
            "1.4613866695827797 0.5547024952015355 1.4613866695827797 0.5547024952015355\n",
            "2.5257745858023126 0.3412698412698413 2.5591318247942416 0.3333333333333333\n",
            "Results for: 8 20\n",
            "1.540995979495036 0.5293969087786645 1.540995979495036 0.5293969087786645\n",
            "2.3144573473776133 0.3492063492063492 2.3144573473776133 0.3492063492063492\n",
            "Results for: 8 30\n",
            "1.5526288900478407 0.5108091726437014 1.5526288900478407 0.5108091726437014\n",
            "2.329983604445754 0.3968253968253968 2.329983604445754 0.39285714285714285\n",
            "Results for: 8 40\n",
            "1.61044140555454 0.5077785634912617 1.61044140555454 0.5077785634912617\n",
            "2.7111649835604217 0.2857142857142857 2.7111649835604217 0.28174603174603174\n",
            "time: 15min 50s\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}